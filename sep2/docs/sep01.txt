>>>filepath:/fcoding/articles/<<<
>>>rensai_name:statbushknife<<<
>>>rensai_number:02<<<
>>>date:2010/8/18<<<
>>>title:第4回 オープン・データを図示・可視化すること(2)<<<
>>>abs（編集部）<<<

●オープン・データを図示・可視化する(2)
今回は前回の続きとして、オープン・データをプロットすることをテーマに取り上げます。前回はRの組込みのグラフ描画関数である、>>>em:plot<<<や>>>em:ts.plot<<<のみを利用しましたが、今回は、外部グラフィックライブラリとして最近非常に注目され、使用頻度が高くなっている>>>em:ggplot2<<パッケージを利用します。この連載でも今後、グラフ描画については>>>em:ggplot2<<<を利用していく予定です。

また今回はwebサービスを利用したデータ取得についてもとりあげます。

ここで一つリマークがあります。前回の記事で齊藤誠さんの「競争の作法」を取り上げ、その検証をしたのですが、そのときの東証PER値のデータについて、十分に層化されていないのにも関わらず、その時間変動を必要以上に取り上げた上で恣意的な解釈を施しているのはおかしいという批判があるようです。これについては後日以下のサポートサイト(>>>link:http://www.bakfoo.com/atmarkit.html<<<)で取り上げたいと思いますが、「競争の作法」ではそのことに対処するために、PER値を直接取り扱うのではなく、エール大学のロバート・シラー氏が提唱する方法を解説しています。それは、個々の企業や業界の収益の短期変動の影響を排除するために、過去10年の実質ベースでみた平均企業業績で当期株価水準の実質値を割った値を利用するという方法です。詳しい解説は「競争の作法」を御覧ください。ただ、この方法を利用しても「いざなみ景気」後半のPER値の高さは際立つという結果になります。また、前回の記事の主旨は、オープン・ソースのソフトウェアを利用して、自分の手でオープン・データを使いこなす初歩を示すことであるために、「過去10年の実質ベースでみた平均企業業績で均す」という比較的煩雑な解析まで踏み込めなかったことをご承知おきください。

●ggplot2という「デフォルト」グラフィックパッケージ
>>>em:ggplot2<<<は、Rを現在よく利用しているユーザの中では、ほぼデフォルトのグラフィックツールとなっているグラフィックパッケージです。ggplot2の設計思想には、SYSTAT Software Incの計算機科学者 Leland Wilkinson氏が提唱する「グラフィック文法 (The Grammar of Graphics)」というものがあります。この「グラフィック文法」は、ユーザを見かけの良いグラフを作成させることに注力させるのではなく、データが本来持っている姿をグラフに反映させることを目指して提唱されているルールです。ユーザが自分の欲しい情報を特定すれば、自動的にソフトウェアが必要な処理をやりグラフを作成するのが本来の姿であり、ユーザがわざわざグラフを作成することをやらせない、というのが「グラフィック文法」が実現した形だそうです。Leland Wilkinson氏は、実際にこの「グラフィック文法」をSPSSベースのライブラリとして開発し、現在ではSPSSの標準機能となっています。「グラフィック文法」の詳細は以下の書籍にありますので、興味がある方は参考になさってください。

The Grammar of Graphics
>>>link:http://www.amazon.com/gp/product/0387245448/<<<

このLeland Wilkinson氏の「グラフィック文法」の精神をRで実現するのが、Rice大学の計算機統計学者Hadley Wickham氏が作成した>>>em:ggplot2<<<です。>>>em:ggplot2<<<の特徴はたくさんありますが、筆者が特に便利と感じているものに以下のものがあります。

>>>ul
・データの何に注目したいかを指定するだけで、デフォルトで美しいグラフを描くことができる
・データや表示の切り口指定を後から追加できるので、グラフ作成をしながらデータを探索できる
・レジェンドと結びついた色分け設定が自動である
・ボックスプロット、散布図、線グラフというグラフの見栄を自由に変更できる
・ファセットグラフ（小さなグラフがタイル状に組み合わさったグラフ）を簡単に作成できる
・複数のデータを一つのグラフにプロットするときであっても、後から足し合わせればよい
・時系列データであっても特別に時系列オブジェクトとして取り扱う必要がない
・審美的 (aesthetic) なグラフィック要素は、後から加えることができる
・デカルト座標も極座標も自在に切り替えることができる
・グラフィックテーマを設定できるので、見栄をドラステックに切り替えることができる
<<<

今回の記事でも色分け設定の自動化やファセットグラフを使いますので、そのときに上記に掲げた特徴が何を言っていたのかを思い起こしてくだされば、と思います。

ここで本来ならば>>>em:ggplot2<<<のチュートリアルとなるわけですが、この>>>em:ggplot2<<<は巨大なライブラリであり、例え入門的な内容であっても、数回の連載では終わりそうにありません。そこで、初歩の初歩をサポートサイト(>>>link:http://www.bakfoo.com/atmarkit.html<<<)に動画としてまとめましたので、これを利用ください。また英語のリスニングやリーディングに問題ない方は、Hadley Wickham氏自身が講義しているビデオがありますので、それを参考になさってください。

Hadley Wickham's Data Visualization using ggplot2 short course
>>>link:http://blip.tv/file/3362248<<<

リファレンスマニュアルはオンライン上にあります。使い方を一度習得すれば、このオンラインリファレンスマニュアルは非常に使い勝手がよいことがわかるはずです。

ggplot2 refrence manual
>>>link:http://had.co.nz/ggplot2/<<<

また、Hadley Wickham氏の書いた>>>em:ggplot2<<<の本もあります。

ggplot2: Elegant Graphics for Data Analysis
>>>link:http://www.amazon.com/gp/product/0387981403<<<

さらにはブログ形式で、>>>em:ggplot2<<<による実例コードを提示しているサイトもあります。

Learning R
>>>link:http://learnr.wordpress.com/<<<

以上のように、現在では書籍やオンライン上のリソースが非常に豊富ですので、初歩の使い方さえ掴めば、あとは若干の探索と試行錯誤でどうにかなると思います。

さて、抽象的なことだけで終わるのもこの連載の主旨に反しますので、ここで具体的に>>>em:ggplot2<<<を利用して、その利用具合の雰囲気を掴みたいと思います。データには前回の最後の日本円の実質実効為替レートを用いて、グラフ表示をしてみることにします。まず、>>>em:ggplot2<<<は標準パッケージでないので、CRANサイトよりインストールする必要があります。以下のRのコマンドで、筑波大学をCRANミラーサイトで選びながらパッケージをインストールしてください。この手順について、サポートサイト(>>>link:http://www.bakfoo.com/atmarkit.html<<<)の動画でも観ることができるようにします。

>>>shell
> install.packages("ggplot2")
<<<

データは前回と同様のGoogle Docsにあるものを利用します。

>>>shell
> data <- read.csv("http://spreadsheets.google.com/pub?key=0AlBuJgqcP5f3dExEQkZfem1MeXFPZTA1UUVaVVZud3c&hl=en&output=csv", header=TRUE, skip=1)
<<<

前回は、この実質実効為替レートのデータを一度時系列オブジェクトに変換してから>>>em:ts.plot<<<で描画しましたが、>>>em:ggplot2<<<を利用する場合はその必要がありません。ただし、日付・時間を表すカラムかを日付オブジェクトに変換する必要があります。以下に、その指定をした後にプロットし、そのグラフを保存する手順を示します。

>>>shell
> data$yearmonth <- paste(data$yearmonth, "/01", sep="")
> data$yearmonth <- as.Date(data$yearmonth, "%Y/%m/%d")
> library(ggplot2)
> f <- ggplot(data = data, aes(x=yearmonth, y=1/stfx*100)) + geom_line(aes(colour="JPY")) + geom_hline(yintercept=1) 
> f + xlab("Year") + ylab("Effective exchange rate index") + opts(title="Effective exchange rate index JPY")  + scale_colour_manual(values=c("JPY"="darkblue"))
> ggsave("japanfx2.png", plot = f)
<<<

このコードの実行結果が次のグラフです。

>>>img:japanfx2.png<<<

グラフを見ると分かるように、Rの>>>em:plot<<<で作成するデフォルトのグラフよりもかなり見かけのよい、綺麗なグラフが描けることがわかります。

このコードの簡単な説明をします。一行目の>>>em:paste<<<関数を利用して"/01"という文字列を「強引」に付け加えているのは、Rの>>>em:date<<<オブジェクトには必ず日付要素が必要だからです。二行目では一行目で加工したものを>>>em:date<<<オブジェクトに変換するために、>>>em:as.Date<<<関数を利用しています。三行目以降がプロットの実際です。コードを見ると分かるように、いろいろなオブジェクトを>>>em:+<<<記号で次々と加え合わせていっています。

まず、>>>em:ggplot2<<<をライブラリとして読み込みます。次に、>>>em:ggplot<<<関数でプロットするべきデータフレームと、プロットするべき変数を指定します。>>>em:ggplot2<<<では、目に触れるものは>>>em:aes<<<関数の中に書くことになっているので、x軸とy軸に何を表示するかという指定はこの>>>em:aes<<<関数で行ないます。>>>em:aes<<<という関数の由来は、"aesthetics（エステティクス、美的要素) "の略です。また、データ点を線で結ぶことを指定するのが>>>em:geom_line<<<関数で、y=1.0の水平線を描くのが>>>em: geom_hline<<<関数です。>>>em:geom_line<<<には、>>>em:aes<<<関数によって、線の色指定をしています。この色指定は実は変数を割り当てるだけで、実際の色は後ほど>>>em:scale_colour_manual<<<関数にて行っています。>>>em: geom_hline<<<では、水平線のy切片を指定するのに>>>em:yintercept<<<引数を利用します。以上のプロットオブジェクトは、>>>em:ggplot2<<<オブジェクトとして>>>em:f<<<という変数に割り当てることにしました。

次の行はx軸、y軸のラベル、グラフのタイトル、そして先程の色変数の色を"darkblue"として決定しています。関数名から明らかなので、敢えて説明はいらないと思います。ポイントは、前の行で作成した>>>em:ggplot2<<<オブジェクト>>>em:f<<<に後から追加的に指定をすればよい、ということです。最後の行は>>>em:ggsave<<<関数でグラフィックを保存します。ファイルは拡張子により自動判定されてpng、jpg,、pdfなどのフォーマットに保存することができます。

以上、簡単に>>>em:ggplot2<<<の手習いをしました。これらの関数の詳細は>>>em:ggplot2<<<のオンラインリファレンスマニュアル（>>>link:http://had.co.nz/ggplot2/<<<)を見てください。実際、ある程度の利用方法を覚えれば、このオンラインリファレンスマニュアルだけで大体のことはできるようになります。

●世界銀行のData Catalog

先月の報道によると、中国が日本のGDPを抜いて世界第二位の経済大国になったそうです。Bloombergの報道によれば、ドル換算の名目値で2010年の第二四半期までの速報値で、日本が1.288兆ドルに対して、中国が1.338兆ドルだそうです。

http://www.bloomberg.com/news/2010-08-16/china-economy-passes-japan-s-in-second-quarter-capping-three-decade-rise.html

このことについては、何年も前から予想されたことでもあるのであまり驚くべきことでもありませんが、オープン・データを利用して確かめてみることにしましょう。ただし、生憎のこと、信頼できるGDPのデータは2008年までのものしかないので、2008年までの傾向をみるだけになることをお断りしておきます。

GDPのデータとして各国のデータが網羅的にまとまっていて、信頼できるものは、世界銀行にあるものか、OECDにあるものか、IMFにあるものかだと思います。

The World Bank Data Catalog
>>>link:http://data.worldbank.org/data-catalog<<<

Gross Domestic Product (GDP) for OECD member countries
>>>link:http://stats.oecd.org/Index.aspx?datasetcode=SNA_TABLE1<<<

IMF World Economic Outlook Database
>>>link:http://www.imf.org/external/pubs/ft/weo/2010/01/weodata/index.aspx<<<

いずれのサイトについてもデータをWebインタフェースから「ドリルダウン」する形にてデータを入手することができます。GDPデータについては、いずれのサイトも2008年までのデータがストアされています。この中で一番簡単の取り扱えて、一番モダンな問い合わせ実装が行われているのが世界銀行のものです。そこで、今回は世界銀行にあるGDPデータを利用します。

>>>img:wbdatabank.jpg<<<

世界銀行の"Data Catalog"は、WDI("the World Development Indicators")というインジケーターをクエリに含めることで、検索を行うデータベースです。WDIはいわばRDBの「テーブル」や「ビュー」にあたります。このWDIには2000を超えるインジケーターが存在します(http://data.worldbank.org/indicator >>>link:http://data.worldbank.org/indicator<<<)。そして、世界銀行の"Data Catalog"はモダンなwebサービスであるRESTインタフェースとしても実装されていることが、データ利用の敷居を極端に下げていると言ってよいでしょう。つまり、HTTPプロトコルを利用でき、RESTを解釈できるシステムなら、誰もが簡単にこのData Catalogを利用できるのです。

Rでこの世界銀行のData CatalogをREST経由で利用するには、>>>em:WDI<<<パッケージを利用します。

WDI: Search and download data from the World Bank's World Development Indicators
>>>link:http://cran.r-project.org/web/packages/WDI/index.html<<<

>>>em:WDI<<<パッケージはCRANの標準パッケージですので、次のコマンドで簡単にインストールすることができます。

>>>shell
install.packages("WDI")
<<<

さて、実際にこの>>>em:WDI<<<を用いて、世界銀行の"Data Catalog"にアクセスし、GDPの米ドルでの名目値、一人当たりのGDPの米ドルでの値、そしてGDPの成長率について、米国、日本、中国、韓国に関する時系列データの比較をしてみたいと思います。以下のコードがそれを実行したものです。

>>>shell
> library(WDI)
> library(ggplot2)

> gdp <- WDI(country = c("US", "JP", "CN", "KR"), indicator = "NY.GDP.MKTP.CD", start = 1960, end = 2008)
> gdppercapita <- WDI(country=c("US", "JP", "CN", "KR"), indicator = "NY.GDP.PCAP.CD", start = 1960, end = 2008)
> gdpgrowth <- WDI(country=c("US", "JP", "CN", "KR"), indicator ="NY.GDP.MKTP.KD.ZG", start = 1960, end = 2008)

> ggplot(gdp, aes(year, NY.GDP.MKTP.CD, color=country))+geom_line(stat="identity") + xlab("Year") + opts(title="GDP (current US$)") + ylab("")
> ggsave(file = 'gdp.png', scale = 0.8)

> ggplot(gdppercapita, aes(year, NY.GDP.PCAP.CD, color=country))+geom_line(stat="identity") + xlab("Year") + opts(title = "GDP per capita (current US$)") + ylab("")
> ggsave(file = 'gdppercapita.png', scale = 0.8)

> ggplot(gdpgrowth, aes(year, NY.GDP.MKTP.KD.ZG, color=country)) + geom_line(stat="identity") + xlab("Year") + opts(title="GDP growth (annual %)")+ylab("")
> ggsave(file = 'gdpgrowth.png', scale = 0.8)
<<<

最初に、>>>em:WDI<<<とグラフ描画用の>>>em:ggplot2<<<を読み込みます。次に、RESTインタフェース経由で世界銀行のData Bankにアクセスするために、>>>em:WDI<<<パッケージを利用します。>>>em:WDI<<<パッケージの使い方は非常に簡単です。単に>>>em:WDI<<<関数を利用し、その引数として>>>em:country<<<、 >>>em: indicator<<<、そして>>>em:start<<<および>>>em:end<<<を指定するだけです。>>>em:country<<<にはデータを入手したい国名をリストで指定し、>>>em: indicator<<<にはWDIのインジケータを指定し、>>>em:start<<<および>>>em:end<<<に検索対象の開始年と終了年を指定します。ここで、>>>em: indicator<<<に指定するインジケータをさがすには、Data Bankのインジケーター検索画面(http://data.worldbank.org/indicator/ >>>link:http://data.worldbank.org/indicator/<<<)を利用して一度検索を行ってから、その表示されるURLの最後の部分に見ればよいでしょう。

>>>img:wdiindicator.jpg<<<

今回のGDPの米ドルでの名目値、一人当たりのGDPの米ドルでの値、そしてGDPの成長率に当たるインジケーターは、>>>em:NY.GDP.MKTP.CD<<<、>>>em:NY.GDP.PCAP.CD<<<、>>>em:NY.GDP.MKTP.KD.ZG<<<になります。>>>em:WDI<<<によって入手できたデータは、例えば名目GDPなら以下のようなレイアウトになっています。

>>>shell
> head(gdp)
        country NY.GDP.MKTP.CD year iso2c
1 United States    1.40933e+13 2008    US
2 United States    1.37416e+13 2007    US
3 United States    1.31165e+13 2006    US
4 United States    1.23641e+13 2005    US
5 United States    1.16309e+13 2004    US
6 United States    1.09080e+13 2003    US
<<<

>>>em:country<<<のカラムは国を表し、>>>em:NY.GDP.MKTP.CD<<<はドル換算にした名目GDP値を表し、>>>em:year<<<が年を表しています。これらをみるとわかるように、これ以上全くデータの加工をすることなしに、そのままRで利用できます。これがRESTインタフェースから直接読み込むことの最大のメリットとなります。

これらの>>>em:WDI<<<からのデータを>>>ggplot<<<で描画して、グラフを保存しているのが、最後の部分のコードです。線グラフを書きたいので、>>>em: geom_line<<<を利用しました。ここで一点注意するのは、>>>em:ggplot<<<関数の引数で入力されている、>>>em:aes<<<の引数です。ここではx軸に>>>em:year<<<を指定し、y軸に>>>em:NY.GDP.MKTP.CD<<<などのインジケーターから取得きる値を指定してプロットするという指示を書いていますが、それと同時に、>>>em:color<<<引数に>>>em:country<<<カラムを指定しています。これは、>>>em:country<<<で指定される国ごとに違う色のプロットをしろという指定です。ここで、名目値でみたドル換算でのGDPのグラフを見てみましょう。

>>> img:gdp.png<<<

これを見ると分かりますように、>>>em:color = country <<<で指定したように、国別に違う色の系列の線グラフとなり、右にレジェンドが自動的に描かれています。このグラフ描画の高度な自動化と簡便さが、まさに>>>em:ggplot2<<<を使う最大のメリットの一つといっていいでしょう。

さてグラフの方ですが、1960年から2008年まで一貫して米国が高い名目GDP値を取っていることがわかります。日本は1995年まではでこぼこながらも強く成長している様がわかります。しかし、1995年から失速して、その後の状態は読者の皆様のご承知の通りです。一方で、中国は1994年以来、力強い成長でをしていて、2008年にはもう一息で日本の名目GDP水準を抜かす状態にあった、ということが見て取れると思います。そして、先程の報道にあるように2010の第二四半期で、日本を名目水準値としては抜かしたということです。

それでは次に、一人当たりのインフレ率で調整された実質GDP値をみてみましょう。

>>>img:gdppercapita.png<<<

1980年代後半に一人当たりの実質GDP値は、日本が米国を抜いていることがわかります。これが「日本の奇跡」だとか「ジャパン・アズ・ナンバーワン」と言われた現象です。しかし、2000年に入って失速してしまい、米国に抜かされていることもわかります。一方で、中国をみますと、一人当たりのGDPに引き直すと韓国よりもまだまだ低い水準に留まっているということがわかります。これは次のコードで入手した各国の人口を見てみれば当然です。

>>>shell
> library(ggplot2)
> library(WDI)
> pop <- WDI(country = c("US", "JP", "CN", "KR"), indicator = "SP.POP.TOTL", start = 1960, end = 2008)
> ggplot(pop, aes(year, SP.POP.TOTL, color=country)) + geom_line(stat="identity") + xlab("Year") + opts(title="Population") + ylab("")
> ggsave(file = 'population.png', scale = 0.8)
<<<

>>> img:population.png<<<

中国は13億人という膨大な人数の人間をかかえているのです。日本と同じだけの一人当たりのGDPを目指そうと思ったら、今の10倍の水準が必要です。

最後は、参考までに、GDPの成長率についてのグラフを提示します。

>>> img:gdpgrowth.png<<<

以上のように、RESTインタフェースを実装されているオープン・データは簡単にデータを入手でき、データを加工する手間も必要なく、Rでスムーズに利用できることがわかったと思います。

と、ここまで議論してきて、一点申し上げたいことがあります。ここであげた世界銀行のデータを表示するくらいならば、実はRでプロットするまでもなく、Googleのpublicdata APIを用いて、すぐに表示することができるということです。Googleのpublicdata APIも世界銀行のWebサービスインタフェースを利用していますので、基本的には同じことをやっていることになります。

http://www.google.com/publicdata?ds=wb-wdi&met=ny_gdp_mktp_cd&tdim=true&dl=en&hl=en&q=world+gdp+statistics#met=ny_gdp_mktp_cd&idim=country:USA:JPN:CHN:KOR

>>>img:gdpGGL.jpg<<<

http://www.google.com/publicdata?ds=wb-wdi&met=ny_gdp_pcap_cd&tdim=true&dl=en&hl=en&q=world+gdp+per+capita+statistics#met=ny_gdp_pcap_cd&idim=country:USA:JPN:CHN:KOR

>>>img:gdppercapitaGGL.jpg<<<

http://www.google.com/publicdata?ds=wb-wdi&met=ny_gdp_mktp_kd_zg&tdim=true&dl=en&hl=en&q=world+gdp+growth+rate+statistics#met=ny_gdp_mktp_kd_zg&idim=country:USA:JPN:CHN:KOR

>>>img:gdpgrowthGGL.jpg<<<

しかし、Rでデータの図示・可視化を行うメリットは、webサービスAPIを公開していないデータソースを利用できることはもちろんですが、それ以上にやはりデータ解析、統計解析ができるという点があります。その例を次に示したいと思います。

●Wikileaksのアフガン戦争ダイアリー

最近のオープン・データ的イベントとして大きかったものは、今年の7月25日にWikileaksが公開した、アフガン戦争ダイアリー(Afgan War Diary)だと思います。これは、いままで数々の機密ファイルや機密映像の暴露を行ってきた民間団体Wikileaksが、米軍からの内部情報としてまとまったレポートを公表したものです。このレポートは、アフガニスタンの駐留米軍の戦闘をイベントごとに記録したもので、2004年初から2009年末の6年間に渡る76,900件のレコードから構成されています。

米国政府の発表ではこの「ダイアリー」自体には軍事的・諜報的な価値がないとされていますが、しかしこれだけ大量の生データが、現在も戦闘及び駐留が継続される作戦から流出したことは前代未聞です。そして、一つ一つの情報の断片だけでは見えないことも、ある切り口からまとまった量のデータをみたときには、新たな情報が見えてこないとは限りません。ただし、今回は単にアフガン戦争ダイアリーのデータをRに取り込み、それれを図示・可視化し、簡単な統計的な解析をするというだけに留めたいと思います。つまり、比較的大きなオープン・データを入手し、取り込み、図示し、解析するという過程をステップバイステップで示したいと思います。

データの解析方法はNew York大学の政治学の大学院生であるDrew Conway氏の方針を採用することにします。実は、筆者もこのWikileaksのデータ公開があった直後に、データをダウンロードしてRにて解析を試みていましたが、Conway氏は筆者より早く解析結果とコードを自身のブログにて公開しました。そしてその後もConway氏は精力的に解析活動を続け、以下のような一連の記事を生み出しています。

Zero Intelligence Agents
http://www.drewconway.com/zia/?p=2226
http://www.drewconway.com/zia/?p=2234
http://www.drewconway.com/zia/?p=2268
http://www.drewconway.com/zia/?p=2295
http://www.drewconway.com/zia/?p=2324

ここでは、Conway氏の解析をベースに、wikileaksのデータを図示・可視化するとともに、筆者も同時期にやっていたベンフォードの法則への割り当てとカイ二乗適合度検定をしてみたいと思います。利用するコードの前半のデータ操作の部分は、筆者のものより洗練されているConway氏のコードを使用させていただき、後半の解析部分は筆者のコードをConway氏のコードにマージする形で示したいと思います。この記事を書く際に、Conway氏には電子メイルにてディスカッションをさせてもらいました。ここにConway氏に感謝いたします。

何よりもまずデータを読み込まなければなりません。この「アフガン戦争ダイアリー」は以下にあり、HTML, CSV, SQL, KMLファイルなどのファイルフォーマットで、誰でもダウンロードが可能な状態になっています。

Wikileaks Afghan War Diary, 2004-2010
>>>link:http://www.wikileaks.org/wiki/Afghan_War_Diary,_2004-2010<<<

これから先以降は、この中のCSVデータをダウンロードして、圧縮を伸長した状態であると仮定します。伸長をするとafg.csvというファイルができるはずです。伸長したファイルのサイズは73.9MBというCSVファイルとしてはかなり大きなものです。そしてそのファイルが"/home/my/"ディレクトリにあると仮定します。

>>>em:setwd<<<関数でアフガン戦争ダイアリーのCSVファイルがあるディレクトリをワーキングディレクトリとし、データを読み込みます。74MBという大きなサイズのCSVですので、読み込みに多少時間がかかるかもしれません。

>>>shell
> setwd("/home/my/")
> afg<-read.csv("afg.csv",stringsAsFactors=FALSE)
> head(afg)
 以下にエラー do.call("data.frame", rval) :  変数名は256バイトが上限です 
<<<

最後の行は変数名に256バイトを超えるオブジェクトがあるので、>>>em:head<<<関数が失敗していました。そこでデータの構造をみるために>>>em:str<<<関数を利用すると、「ダイアリー」のデータは32カラムで76910行のデータフレームになっていることがわかります。

>>>shell
> str(afg)
'data.frame':	76910 obs. of  32 variables:
 $ D92871CA.D217.4124.B8FB.89B9A2CFFCB4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    : chr  "C592135C-1BFF-4AEC-B469-0A495FDA78D9" "D50F59F0-6F32-4E63-BC02-DB2B8422DE6E" "E3F22EFB-F0CA-4821-9322-CC2250C05C8A" "4D0E1E60-9535-4D58-A374-74367F058788" ...
<<<
 
 このデータフレームのカラムにカラム名を追加します。そのために「ダイアリー」のデータ構造について調べる必要があります。このために、http://wardiary.wikileaks.org/>>>link:http://wardiary.wikileaks.org/<<<にあるボールド体による"the structure of the report"を参考にしましょう。
 
これを眺めてみると、まず俯瞰的な描像を手に入れたいと思うはずです。そのために、「6年間に渡って毎月どのくらいのイベントがどこで生じたのか」ということを図示することにします。それに必要なカラムはどうやら、イベントが起きた日付データの>>>em:DateOccurred<<<、場所を表す>>>em:Region<<<、イベントの種類（友軍への攻撃、敵への攻撃等）を表す>>>em:AttackOn<<<（これは>>>em:Affilication<<<でも代替できそうです）くらいでしょうか。ちなみに、この「ダイアリー」の省略コードにある、KIAは"Killed in Action", WIAは"Wounded in Action"だそうです。そして、FriendlyWIAは友軍が負傷、CivilianKIAは一般市民が死亡人数を表すそうです。これで、攻撃の種類による負傷者数や死者数を知ることができそうです。その他の略字は、先程のリンク先の説明を参考にしてください。

さて、「ダイアリー」のデータ構造がわかったところで、読み込んだデータフレームに、上記のサイトにあるカラム名を追加します。
 
>>>shell
> colnames(afg)<-c("ReportKey", "DateOccurred", "Type", "Category", "TrackingNumber", "Title", "Summary", "Region", "AttackOn", "ComplexAttack", "ReportingUnit", "UnitName", "TypeOfUnit", "FriendlyWIA", "FriendlyKIA", "HostNationWIA", "HostNationKIA", "CivilianWIA", "CivilianKIA", "EnemyWIA", "EnemyKIA", "EnemyDetained", "MGRS", "Latitude", "Longitude", "OriginatorGroup",  "UpdatedByGroup", "CCIR", "Sigact", "Affiliation", "DColor", "Classification")
> head(afg)
                             ReportKey        DateOccurred                Type            Category
1 C592135C-1BFF-4AEC-B469-0A495FDA78D9 2004-01-01 00:00:00     Friendly Action Cache Found/Cleared
2 D50F59F0-6F32-4E63-BC02-DB2B8422DE6E 2004-01-01 00:00:00    Non-Combat Event          Propaganda
3 E3F22EFB-F0CA-4821-9322-CC2250C05C8A 2004-01-01 00:00:00        Enemy Action         Direct Fire
4 4D0E1E60-9535-4D58-A374-74367F058788 2004-01-01 00:00:00     Friendly Action Cache Found/Cleared
5 84A769E7-73B4-4B30-AD36-06497F766F15 2004-01-02 00:00:00 Suspicious Incident        Surveillance
6 B81BE975-9081-4D19-A8FE-C2DB0C18A095 2004-01-02 00:00:00 Suspicious Incident        Surveillance
        TrackingNumber                      Title
1 2007-033-004738-0185 CACHE FOUND/CLEARED  Other
2 2007-033-010818-0798          PROPAGANDA  Other
3 2007-033-004042-0850         DIRECT FIRE  Other
4 2007-033-004738-0279 CACHE FOUND/CLEARED  Other
5 2007-033-010701-0111                SURV  Other
6 2007-033-010701-0220                SURV  Other
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Summary
1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       USSF FINDS CACHE IN VILLAGE OF ....
<<<
 
コードでは>>>em:colnames<<<を利用してカラム名を付け、>>>em:head<<<で最初の6行を取り出しました。ここでは長くなりますので、Summaryの先頭部分以降は省略しています。Summaryに長い文字のレポートが入っていることが分かります。どうやら、最初の>>>em:head<<<関数がミスをしたのはここをタイトルとして読み込もうとしたことが理由のようです。

さて、ここで「6年間に渡って毎月どのくらいのイベントがどこで生じているか」という図示をするために、日付データや地域データを整形します。日付データは>>>as.Data<<<としてdateオブジェクトに変換します。幸い、このCSVの>>>em:DateOccurred<<<にあるデータはUnix形式の日付データなので、そのまま素直にはいります。また、地域を表す>>>em:Region<<<は因子として扱いたいので>>>as.factor<<<でfactorオブジェクトに変換します。

>>>shell
> afg$DateOccurred <- as.Date(afg$DateOccurred)
> afg$Region<-as.factor(afg$Region)
<<<

さて、ここまでで>>>em:afg<<<オブジェクトには必要なデータが全て揃っていることになります。それではあらためて問いますが、「6年間に渡って毎月どのくらいのイベントがどこで生じているか」を図示するためにはどうすればいいでしょう？まず、読み込むデータフレームは>>>em:afg<<<であり、日付のx軸は>>>em:DataOccrred<<<である。そして、これを30日ごとのヒストグラムとして表す。しかし、ヒストグラムはイベントの種類>>>em:AttackOn<<<もしくは>>>em: Affiliation<<<によって色分けした積み上げグラフにし、しかも、これら全てについて地域を表す>>>em:Region<<<ごとに書き出したい。これを実行するのは大変なようです。しかし、>>>en:ggplot2<<<を使用し、このまま「グラフィック文法」に沿ってコーディングすれば、この作業はいとも簡単にできてしまいます。

以下にそのコードを示します。

>>>shell
> library(ggplot2)
> event <- ggplot( afg, aes(x = DateOccurred) ) + stat_bin( aes( y = ..count.., fill = AttackOn ), binwidth=30 ) + facet_wrap( ~Region ) + 
    opts( title = "Monthly Event Report per Region and Target" ) + xlab( "Date") + ylab( "Report Counts" ) + 
    scale_fill_manual( values = c( "darkred", "darkblue", "darkgreen", "orange" ), name = "AttackOn" ) + scale_x_date( major = "years", minor = "months" )
> ggsave("wikileaks_event.png", plot = event, width=10, height=5)
<<<

>>>em:ggplot<<<でデータフレームとx軸を指定し、>>>em:stat_bin<<<でヒストグラムを描きます。>>>em:stat_bin<<<の>>>em:binwidth<<<は30日ごとのヒストグラムにしたいので30としています。また、>>>em:stat_bin<<<の>>>em:aes<<<関数には、y軸はカウントデータを表す>>>em:...cont...<<<を、色分けは>>>em:AttackOn<<<で行うのでそれを>>>em:fill<<<の引数としました。最後の>>>em:Region<<<ごとに書き出すのは>>>em:ggplot2<<<の「十八番」であるファセットグラフを利用します。>>>em:facet_wrap<<<関数がそれです。
 
 こうして書き出したグラフは次ようになります。
 
 >>>img: wikileaks_event.png<<<
 
これを見ると分かるのは、戦闘は全ての地域で行われるのではなく、タリバン勢力の支配地域となっている東および南に多いということです。また、各々のファセットグラフの時間変動をみると季節性の変動があることが分かります。アフガニスタンは冬が厳しいためです。更に、時間を経るごとに戦闘数が多くなっていることがわかります。つまりアフガニスタン戦争が終結した後も、内戦状態が増々激しさを増している状況が想像できます。先程、2011年7月から米軍は撤退を開始するという報道がありましたが、果たしてこの状態で米軍は来年から撤退を開始できるでしょうか？この解析からは疑問符が付くように思えますが、それは杞憂でしょうか？
 
まだ、解析は始まったばかりなのですが、紙面の都合上、次に進みたいと思います。次に検証を試みたいのは、果たしてこのデータは本物なのかどうか、ということです。もちろん、既に米国政府はこのデータは正式なものであると認めていますので、それを信じるならばこれ以上の解析は必要ありません。しかし、ここではその判断を一旦留保しまして、データが人為的に作成された可能性があるかについて調べたいと思います。

ここで「ベンフォードの法則」が登場します。「ベンフォードの法則」は、データが人為的に作成されたかどうかを検証するための有用なツールです。その利用の概要を一言で述べますと、自然界に存在する多くの数字や数値に対して最初の一桁だけを取り出して数えた場合、それがある法則( log10((1+x)/x )に基づいているということを利用するのです。有名なのが、経理データで不正経理をしているとおぼしき数値の先頭桁だけを取り出しヒストグラムにした場合、ベンフォードの法則に従っていない場合は人がデッチ上げをした可能性がありそうだ、というものです。実際に、不正な会計操作を発見した実例が以下のエッセイに書いてあります。英語が億劫でなければ読んでみてください。

I've Got Your Number
http://www.journalofaccountancy.com/Issues/1999/May/nigrini.htm

その他の詳細は、ウィキペディアを参考にするか、筆者も昔個人ブログに記述したことがあったので、それを参考にしてください。

ウィキペディア: ベンフォードの法則
http://ja.wikipedia.org/wiki/%E3%83%99%E3%83%B3%E3%83%95%E3%82%A9%E3%83%BC%E3%83%89%E3%81%AE%E6%B3%95%E5%89%87

wrong, rogue and log: Benfordの法則 -その1-
http://kashino.exblog.jp/7309447/
wrong, rogue and log: Benfordの法則 -その2-
http://kashino.exblog.jp/7320241/

さて、このベンフォード則の「ダイアリー」への適用ですが、先程の図示化した月ベースではデータ数が少なすぎ、日ベースではデータの種類が少なすぎるので、都合がよくなさそうです。ここでは、週ごとにデータをまとめることにします。実は、これはすごく簡単で、すでにRのdateオブジェクトになっているので、>>>format.Date<<<関数を利用してフォーマットを指定するだけです。

>>>shell
> weeklyCount<-cbind(table(cbind(format.Date(afg$DateOccurred,"%Y %W"))))
> head(weeklyCount)
        [,1]
2004 00   12
2004 01   27
2004 02   43
2004 03   31
2004 04   27
2004 05   16
<<<

最終的にデータフレームで利用するために縦ベクトルであって欲しいので、>>>em:cbind<<<、>>>em:table<<<、>>>em:cbind<<<という姑息な手段を利用しました。さて、この>>>em:weeklyCount<<<の先頭桁を取り出すには、工夫が必要そうです。幸いC言語の数値フォーマットに変換する関数>>>em:formatC<<<を利用すれば、例えば、

>>>shell
> formatC(12, format="e")
[1] "1.2000e+01"
<<<

のようにできるので、これを文字列の部分文字列取り出し関数>>>em:substring<<<を利用すればよさそうです。以下にコードを示しました。

>>>shell
> pullLeaddigit<-function(x) { as.numeric(substring(formatC(x, format = "e"), 1, 1)) } 
> head(pullLeaddigit(weeklyCount))
[1] 1 2 4 3 2 1
<<<

実際に適用すると、良好なようです。さて、この>>>em:pullLeaddigit<<<関数をつかって、>>>em:weeklyCount<<<の先頭桁を取り出し、それを散布図プロットをして、ベンフォードの法則の理論曲線と比べてみましょう。

>>>shell
> digits <- table(pullLeaddigit(weeklyCount))
> digits <- cbind(digits)
> digits <- as.data.frame(digits)
> colnames(digits) <- "digitCount"
> digits$digitCount <- digits$digitCount/sum(digits$digitCount)
>
> dbenford <- function(x) { return(log10((1+x)/x)) }
> 
> benford <- ggplot(digits, aes(x=1:9, y=digitCount)) + geom_path(aes(colour = "observation")) + geom_point(aes(colour = "observation")) + stat_function(fun = dbenford, aes(colour = "theory")) + scale_colour_manual( values = c("observation" = "orange", "theory" = "blue")) + xlab("Digit") + ylab("Count Density") + opts(title="Applying Benford's law to Wikileaks Data") 
> ggsave("wikileaks_benford.png", plot=benford, width=7, height=7)
<<<

以下が出力されるグラフです。

>>>img: wikileaks_benford.png<<<

オレンジがデータ、青がベンフォードの法則の理論曲線です。これをみるとわかるように、1が理論曲線に比べて少ない気はしますし、2は多すぎるような気がしますが、大局的にはよく当たっているように思います。

さて、それが本当に統計学的に言えるかどうかを調べるには、どうすればよいでしょうか？そうです、第一回、第二回で取り上げた統計的検定のカイ二乗適合度検定を用いればいいのです。この場合の帰無仮説は、「アフガン戦争ダイアリーの週データの先頭桁カウントは、ベンフォード則の分布と同じである」です。>>>em:chisq.test<<<を利用して早速しらべてみましょう。

>>>shell
> chisq.test(digits$digitCount, dbenford(1:9))

	Pearson's Chi-squared test

data:  digits$digitCount and dbenford(1:9) 
X-squared = 72, df = 64, p-value = 0.2303

 警告メッセージ： 
In chisq.test(x = digits$digitCount, y = dbenford(1:9)) :
   カイ自乗近似は不正確かもしれません 
<<<

結果を見ると分かるように、>>>em:p-value = 0.2303<<<である、つまり有意水準5%でも帰無仮説は棄却できないということです。これは言い換えると、「アフガン戦争ダイアリーの先頭桁カウントは、ベンフォード則の分布と同じである」ことを>>em:否定するのは統計的に言えなさそうだ<<<ということです。以上、ベンフォード則とのグラフをみても、カイ二乗適合度検定をみても、アフガン戦争ダイアリーは人が捏造したものでなさそうだ、ということです。

以上、かなり重たい内容になってしまいましたが、このようにデータさえ手にいれれば、データ解析を自分の手でできることが感覚的にわかったかと思います。

●次回について

今回は、>>>em:ggplot2<<<の紹介、webサービス経由でデータリポジトリを利用する方法、そして、それなりに大きなデータをどのように扱って可視化し、解析するか、ということについて実践しました。次回はオープン・データを図示・可視化する際の注意点をまとめると共に、統計学的な推測や予測についての初歩についてお話したいと思います。それでは、二週間後にまたお目にかかりましょう。


