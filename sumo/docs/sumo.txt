>>>filepath:/fcoding/articles/<<<
>>>rensai_name:statbushknife<<<
>>>rensai_number:02<<<
>>>date:2010/11/03<<<
>>>title:第8回 大相撲のアノーマリー (1)<<<
>>>abs
（編集部）<<<

●今回の前口上

諸事情により一ヶ月半の間連載を空けてしまいました。申し訳ございませんでした。実は10月、11月の初めに用意した時事ネタがいろいろあるのですが、こちらは少し時代遅れになりましたので、もう少し寝かしてから提示したいと思います。また、何人かの読者の方から励ましのお言葉を戴きまして大変感謝しています。本当に励みになります。もしも取り上げて欲しい問題やネタ、改善点などがありましたら遠慮無く以下のメイルアドレスご意見をお送りください。

knife@bakfoo.com

今回から数回は、大相撲の統計データをネタにして公になっているが混乱しているデータ(英語では"messy data"といいます）を如何にしてキレイにし、キレイにしたデータをどのように解析するかという一連の手順を提示します。データ解析を実務で行う場合には、いわばここが一番の肝の部分になり、それを具体的な事例を用いて解説しようと思っています。

●汚いデータをどうする？

この連載ので以前紹介した世界銀行のWeb APIサービスは、webにおける現在最も洗練されたデータ公開の方法の一つです。APIの仕様さえ守ればRESTインタフェースから誰でも必要な情報にアクセスでき、データを入手することが可能です。しかし、すべての組織や個人がこのように洗練した形でデータを公開しているわけではありません。むしろ、ほとんどのwebにある情報は、コンピュータの視点からみると混乱している汚いデータである、といっていいでしょう。

混乱している汚いデータの筆頭として、構造化を考えて記述されていないHTMLによるデータがあります。ポータルサイトにあるスポーツの勝敗表などがその代表例です。このようなwebにある情報を入手して必要なデータを取り出すことを、「HTMLスクレイピング」と言います。特定のURLやHTMLに依存して目的のデータを探すわけですから、スクレイピング対象のwebサイトのシステム変更やHTMLの仕様変更にとても弱く、スクレイピングプログラムはメンテナンスが大変ですぐに利用ができなくなりがちです。

最近はPerlのMechanizeやRubyのscrubytなどの高機能なスクレイピングフレームワークが出ていますが、そのスクレイピングフレームワークを利用しても、スクレイピングのプログラムはHTMLの特定タグや語をテキスト解析しながらプログラミングすることが必要です。変更に弱く、容易にプログラミングしにくいことは変わりません。

ここで問題を切り分けましょう。汚いデータがあるのは事実と認め、そのデータが容易に変更されることも事実と認めましょう。この二つの要件は所与であると。それでは、それらのデータを変更に対応できるように解析することについて、効率化を図ることにします。

仕様が変更になりがちな汚いHTMLを効率よくテキスト解析してデータを入手する。そのためには、探索的にHTMLを解析できること、一度解析した手順を何度も使い回せること、という二つの要件が必要です。今回はこの要件を満たすためのシステムとして、Google Refineを利用しましょう。

そして汚いデータの筆頭としてYahoo! Japanにある大相撲の取組結果をみます。このデータを取り上げたのは二つの理由があります。一つはHTMLの構造的に混乱しているのでテキスト解析のプログラミングが面倒なこと、もう一つは大相撲の取り組み結果は非常に面白い解析結果が知られていることです。

●Yahoo! Japanスポーツの大相撲の取り組み結果

Yahoo! Japanスポーツにおける大相撲の取り組み結果の表示は以下のとおりです。(http://sports.yahoo.co.jp/sumo/etc/torikumi/199901/>>>link:http://sports.yahoo.co.jp/sumo/etc/torikumi/199901/<<<)
>>>img:y!sumo.jpg<<<

このHTMLは以下のようになっています。

>>>code
...
<table border=0 cellpadding=2 cellspacing=1 width=95%>
<tr><td align="right">
☆は勝ち越し、★は負け越し
</td></tr>
</table>
<table border=3 cellpadding=2 cellspacing=1 width=95% bgcolor="#F5DEB3">
<tr>
<th width=48%><font size="+1">東</font></th>
<th width=4% nowrap>格<br>付</th>
<th width=48%><font size="+1">西</font></th>
</tr>
<tr align="center" valign="top">
  <td>
    <b>貴乃花　☆</b><br>
    8勝7敗<br>
    <table border=0 cellpadding=1 cellspacing=1>
      <tr align=center valign=top>
        <td>●<br>琴<br>錦</td>
        <td>●<br>土<br>佐<br>ノ<br>海</td>
        <td>○<br>魁<br>皇</td>
        <td>●<br>玉<br>春<br>日</td>
        <td>○<br>蒼<br>樹<br>山</td>
        <td>○<br>湊<br>富<br>士</td>
        <td>○<br>栃<br>東</td>
        <td>○<br>闘<br>牙</td>
        <td>○<br>出<br>島</td>
        <td>●<br>琴<br>竜</td>
        <td>○<br>時<br>津<br>海</td>
        <td>●<br>武<br>双<br>山</td>
        <td>○<br>琴<br>乃<br>若</td>
        <td>●<br>千<br>代<br>大<br>海</td>
        <td>●<br>武<br>蔵<br>丸</td>
      </tr>
    </table>
  </td>
...
<<<

番付表と同じ見目を保持させるためか、意味的に異なる勝敗の星と対戦相手が同じセルに書かれていて、対戦相手の名前も<br>タグで見かけ上縦書きにしています。もちろん、このくらいのHTMLなら正規表現を普通に使える方やScrubytの使い手でしたら、少し頑張れば試行錯誤しながらパースできるかもしれませんが、もっと簡単にパースできる方法があるとすればそれを使わない手はありません。

●大相撲のアノーマリー

世界的ベストセラーになった"Freaknomics"(邦訳「ヤバい経済学」)で「相撲の八百長疑惑」として知られましたが、その元ネタとなったのが以下のMark Duggan と Steven D. Levittの論文です。

”Winning Isn't Everything: Corruption in Sumo Wrestling" >>>link: http://pricetheory.uchicago.edu/levitt/Papers/DugganLevitt2002.pdf<<<

この論文でDugganとLevittは1989年から2000年までの十両以上の取組表、70程度の力士の対戦を解析して、不自然なアノーマリーを報告しました。それは、7勝8敗の力士の数が非常に少ないというものです。普通大相撲は一人の力士の取り組み数は場所あたり15回です。すると、勝ち越し・負け越しが決まるのは7勝8敗と8勝7敗のラインで、普通のランダムな対戦を考えるとどちらも同じ程度の出現数になるはずです。しかし、論文のFigure2をみると、明らかに7勝8敗が少なくて、8勝7敗が多いという「アノーマリー」が生じています。

>>>img:dugganLevvitf2.jpg<<<

論文はこのアノーマリーを更に詳しくみて、最終日の対戦において7勝7敗の力士が、すでに勝ち越しを決めている力士と対戦するときに、7勝7敗の力士が勝つ確率が高いこと、その他の起こりうる仮説を立て、仮説検定しても勝ち越し・負け越しのギリギリの当落線上にいる力士が勝ちやすいという傾向を統計解析で明らかにします。そして、ここからなぜ当落線上の力士が勝ちやすいかということを、経済学的なインセンテティブ構造からアネクドート的に記述します。ここでは詳しくは書きませんが、大変面白い論文なので機会があったら読むことを是非オススメします。

上記の論文は1989年から2000年のデータを利用していました。それでは、最近のデータを利用すればどうなるか？ということが当然気になります。それをこのシリーズで順を追って見ていきましょう。あまり隠すのも趣味が悪いので、先に結果だけを提示していきます。それは以下のグラフになります。

>>>img: sumoanorm.png<<<

横軸が勝ち星の数で、縦軸が実際にその勝ち星を稼いだ力士の人数です。赤の線が1999年1月場所から2010年9月場所までの勝敗結果の分布で、青緑の線が勝負がランダムに生じたと仮定したときに描かれる二項分布になります。これをみるとわかるように、本来は500人ほどいてもよい7勝8敗の力士数が、350人程度というようにかなり少なくなっています。また、DugganとLevittの結果より、負け星が極端に多い力士と勝ち星が極端に多い力士の人数が多くなっていて、裾が広い分布になっています。このシリーズでは、まずはこの「大相撲のアノーマリー」を自分で確かめることを試みます。

●Google Refine

しかし、ここで問題が生じます。大相撲のデータは、世界銀行のRESTインタフェースのような、洗練したデータリポジトリから簡単に入手できるわけではない、ということです。そこで今回は前述したようにYahoo! Japanスポーツの大相撲の取り組み結果をスクレイピングして、データを入手することにします。そのために用いるのが、Google Refineです。

http://code.google.com/p/google-refine/>>>link:http://code.google.com/p/google-refine/<<<

>>>img:refine.jpg<<<

Google Refineは汚いデータをクリーニング/クレンジングするツールとしてGoogleのFreebaseチームがリリースしたオープンソース・ソフトウェアです。モットーが"Google Refine, a power tool for working with messy data"とあるように、汚い混乱したデータをキレイにするツールなのです。

Google Refineの特徴は、以下のとおりです。

>>>item
・インタラクティブに結果を見ながら探索的にデータクリーニングができる。
・データクリーニングをするには、スプレッドシートにフィルタを書けたり、MS Excelで言うところのピボットテーブルを配置する感じであるが、もっと分かりやすくインタラクティブなインタフェースである。
・クリーニングにはスクリプトも利用するが、そのスクリプトにはPython(Jython)、 Closure、あと独自の言語Google Refine Expression Language(GREL)を利用する。
・スクリプトを書けば、リアルタイムにドライランして、その結果を例示してくれる。
・クリーニングの手順はJSONとして記録され、JSONのテキストとして入手できる。それを別のプロジェクトに適用することも可能。
・クリーニングの手順はどのステップにも戻ることができるし、Undo/Redoも自在である。
・スクリプト上にあるオブジェクトはすべてJSON経由で外部のWeb Service APIを利用することができる。例えば住所のカラムがあるときに、それをGoogle Map APIを利用して、セルにある住所から位置情報を取得するということが容易にできる。
・ローカルマシンで動作するwebサーバーであり、クライアントにブラウザを利用する。
・クライアント・サーバー間の通信はWeb技術の標準であるHTTPであり、JSONを利用して構造データを受け渡しする。
・データのクラスタリングアルゴリズムが組み込まれていて、文字列の一致検索だけではなく、類似検索もできる。
・Javaベースである。
・Google Freebaseのスピンアウトプロジェクトであるためか、Freebaseと相性がかなり良く、Freebaseからのインプット・アウトプットが容易である。
<<<

Google Refineは本来は、テータクリーニングのために使うために開発されましたが、この柔軟で強力なツールをHTMLのwebスクレイピングには使わない手はありません。ここでは、まずはイントロとしてGoogle Refineの手習いをしましょう。

Google Refineのダウンロードは以下のURLから行います。

http://code.google.com/p/google-refine/wiki/Downloads >>>link:http://code.google.com/p/google-refine/wiki/Downloads<<<

ぞれぞれのOSに対応したアーカイブをダウンロードし、アーカイブされたファイルを解凍して適切なフォルダに配置し、Refine.exeをダブルクリック(MacはGoogle Refine.appをダブルクリック、Linuxは"./refine"でコマンドラインから起動すればGoogle Refineのサーバーがローカルマシン上で立ち上がります。後は、http://127.0.0.1:3333/ にアクセスすれば、Google Refineにアクセスできます。

以下では起動できたと仮定して、実際にRefineを使ってYahoo!スポーツの大相撲取り組みデータを「クリーニング」する手順の概略を説明します。といいましても、Refineの操作はほとんどGUIですので、ここで詳細に文章で記述をしてもなかなか掴めないと思います。そこで、今回のRefineを使って探索的にYahoo! Sportsの大相撲取組表をスクレイピングする模様を動画にしましたので、それを御覧ください。

動画：refineを使ったデータクリーニング >>>link:http://www.bakoo.com/atmarkit/refine.html<<<

動画で使用したHTMLファイルは以下にあります。

https://github.com/yutakashino/atmarkIT/raw/master/sumo/codes/yahooSportsUTF/199901.utf.txt >>>link:https://github.com/yutakashino/atmarkIT/raw/master/sumo/codes/yahooSportsUTF/199901.utf.txt<<<

このファイルをRefineに読み込み、新規プロジェクトとして登録したのちに、以下のJSONスクリプトを「Undo/Redoタブ＞Apply...」ボタンで出現するテキストエリアにコピーして、実行すれば必要なデータを手に入れることができます。

https://github.com/yutakashino/atmarkIT/raw/master/sumo/codes/refineSumoJson.txt >>>link:https://github.com/yutakashino/atmarkIT/raw/master/sumo/codes/refineSumoJson.txt<<<

>>>img:refineApply.jpg<<<

基本的にこのJSONスクリプトがあれば、Yahoo!スポーツの取組表HTMLから必要なデータを抽出できます。しかし、HTMLの数は、1999年1月から2010年9月まで一年あたり7場所ありますから計71個になります。これを手でやるのはさすがに大変なので、これを自動化できないかと考えるのが自然です。

幸いGoogle RefineはHTTPを受け取るWebサーバーですので、HTTPを話すスクリプトでRefineを外から操作することは容易なのです。それをこの後に解説したいのですが、今回は紙幅の関係でここで一旦中断して、HTMLクリーニングの自動化は次回に持ち越しすることにします。

●次回について

次回もこの「大相撲のアノーマリー」をテーマにしたいと思います。今回行った作業を自動化して外のスクリプトから操作させ、目的のデータを入手することを中心に行います。また、Rを利用してそのデータを操作し解析することを行います。それでは、またお会いましょう。