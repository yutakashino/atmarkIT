>>>filepath:/fcoding/articles/<<<
>>>rensai_name:statbushknife<<<
>>>rensai_number:08<<<
>>>date:2010/12/01<<<
>>>title:第8回 大相撲のアノーマリー (1)<<<
>>>abs
（編集部）<<<

●今回の前口上

諸事情により二ヶ月の間連載を空けてしまいました。申し訳ございませんでした。実は10月、11月に用意した時事ネタがいろいろあるのですが、こちらは少し時代遅れになりましたので、もう少し寝かしてから提示したいと思います。また、何人かの読者の方から励ましのお言葉を戴きまして大変感謝しています。本当に励みになります。ありがとうございました。もしもこの連載で取り上げて欲しい問題やネタ、改善点などがありましたら遠慮無く以下のメイルアドレスにリクエストやご意見をお送りください。

knife@bakfoo.com

今回から数回は、大相撲の統計データをネタにして公になっているが混乱しているデータ(英語では"messy data"といいます）を如何にしてキレイにし、キレイにしたデータをどのように解析するかという一連の手順を提示します。データ解析を実務で行う場合には、いわばここが一番の肝の部分になり、それを具体的な事例を用いて解説しようと思っています。

●汚いデータをどうする？

この連載ので以前紹介した世界銀行のweb APIサービスは、webにおける現在最も洗練されたデータ公開の方法の一つです。APIの仕様さえ守ればRESTインタフェースから誰でも必要な情報にアクセスでき、データを入手することが可能です。しかし、すべての組織や個人がこのように洗練した形でデータを公開しているわけではありません。むしろ、ほとんどのwebにある情報は、コンピュータの視点からみると混乱している汚いデータである、といっていいでしょう。

混乱している汚いデータの筆頭として、構造化を考えて記述されていないHTMLによるデータがあります。ポータルサイトにあるスポーツの勝敗表などがその代表例です。このようなwebにあるHTML等で表現された情報を入手して必要なデータを取り出すことを、「HTMLスクレイピング」と言います。スクレイピングは一見するとwebを効率よく利用しているように見えますが、特定のURLやHTMLに依存して目的のデータを探すわけで、スクレイピング対象のwebサイトのシステム変更やHTMLの仕様変更にとても弱く、スクレイピングプログラムはメンテナンスが大変ですぐに利用ができなくなりがちです。

最近はPerlのWWW::MechanizeやRubyのscrubytなどの便利で高機能なスクレイピングフレームワークが出て各方面で積極的に利用されています。しかし、便利で高機能なスクレイピングフレームワークを利用しても、スクレイピングのプログラムはHTMLの特定タグや語をテキスト解析しながらプログラミングすることが必要です。変更に弱く、容易にプログラミングしにくいことは変わりません。

ここで問題を切り分けましょう。汚いデータがあるのは事実と認め、そのデータが容易に変更されることも事実と認めましょう。この二つの要件は所与であると。汚いデータが容易に変更されたとしても、その変更に対応できるような手法をみつけるということにフォーカスします。

仕様が変更になりがちな汚いHTMLを効率よくテキスト解析してデータを入手する。そのためには、探索的にHTMLを解析できること、一度解析した手順を何度も使い回せること、という二つの要件が必要です。今回はこの要件を満たすためのシステムとして、Google Refineを利用しましょう。

そして汚いデータの筆頭として、Yahoo! Japanにある大相撲の取組結果を取り上げます。このデータを取り上げたのは二つの理由があります。一つはHTMLの構造的に混乱しているのでテキスト解析のプログラミングが面倒なこと、もう一つは大相撲の取り組み結果は非常に面白い解析結果が知られていることです。

●Yahoo! Japanスポーツの大相撲の取り組み結果

Yahoo! Japanスポーツにおける大相撲の取り組み結果の表示は以下のとおりです。(http://sports.yahoo.co.jp/sumo/etc/torikumi/199901/>>>link:http://sports.yahoo.co.jp/sumo/etc/torikumi/199901/<<<)
>>>img:y!sumo.jpg<<<

このHTMLは以下のようになっています。

>>>code
...
<table border=0 cellpadding=2 cellspacing=1 width=95%>
<tr><td align="right">
☆は勝ち越し、★は負け越し
</td></tr>
</table>
<table border=3 cellpadding=2 cellspacing=1 width=95% bgcolor="#F5DEB3">
<tr>
<th width=48%><font size="+1">東</font></th>
<th width=4% nowrap>格<br>付</th>
<th width=48%><font size="+1">西</font></th>
</tr>
<tr align="center" valign="top">
  <td>
    <b>貴乃花　☆</b><br>
    8勝7敗<br>
    <table border=0 cellpadding=1 cellspacing=1>
      <tr align=center valign=top>
        <td>●<br>琴<br>錦</td>
        <td>●<br>土<br>佐<br>ノ<br>海</td>
        <td>○<br>魁<br>皇</td>
        <td>●<br>玉<br>春<br>日</td>
        <td>○<br>蒼<br>樹<br>山</td>
        <td>○<br>湊<br>富<br>士</td>
        <td>○<br>栃<br>東</td>
        <td>○<br>闘<br>牙</td>
        <td>○<br>出<br>島</td>
        <td>●<br>琴<br>竜</td>
        <td>○<br>時<br>津<br>海</td>
        <td>●<br>武<br>双<br>山</td>
        <td>○<br>琴<br>乃<br>若</td>
        <td>●<br>千<br>代<br>大<br>海</td>
        <td>●<br>武<br>蔵<br>丸</td>
      </tr>
    </table>
  </td>
...
<<<

番付表と同じ見目を保持させるためか、意味的に異なる勝敗の星と対戦相手が同じセルに書かれていて、対戦相手の名前も<br>タグで見かけ上縦書きにしています。もちろん、このくらいのHTMLなら正規表現を普通に使える方やScrubytの使い手でしたら、少し頑張れば試行錯誤しながらパースできるかもしれませんが、もっと簡単にパースできる方法があるとすればそれを使わない手はありません。

●大相撲のアノーマリー

世界的ベストセラーになったSteven D. LevittとStephen J. Dubnerの"Freaknomics"(邦訳「ヤバい経済学」)では、「相撲の八百長疑惑」が話題になりました。その元ネタとなったのが以下のMark Duggan と Steven D. Levittの論文です。

”Winning Isn't Everything: Corruption in Sumo Wrestling" >>>link: http://pricetheory.uchicago.edu/levitt/Papers/DugganLevitt2002.pdf<<<

この論文でDugganとLevittは1989年から2000年までの十両以上の取組表、70程度の力士の対戦を解析して、不自然なアノーマリーを報告しました。それは、7勝8敗の力士の数が非常に少ないというものです。普通大相撲は一人の力士の取り組み数は場所あたり15回です。すると、勝ち越し・負け越しが決まるのは7勝8敗と8勝7敗のラインで、普通のランダムな対戦を考えるとどちらも同じ程度の出現数になるはずです。しかし、論文のFigure2をみると、明らかに7勝8敗が少なくて、8勝7敗が多いという「アノーマリー」が生じています。

>>>img:dugganLevvitf2.jpg<<<

論文はこのアノーマリーを更に詳しくみて、最終日の対戦において7勝7敗の力士が、すでに勝ち越しを決めている力士と対戦するときに、7勝7敗の力士が勝つ確率が高いことを統計解析により示しました。また、この現象を説明できる可能性のある仮説を立て、仮説検定をすると、やはり勝ち越し・負け越しのギリギリの当落線上にいる力士が勝ちやすいという傾向があることを明らかにしました。そして、ここからなぜ当落線上の力士が勝ちやすいかということを、経済学的なインセンテティブ構造からアネクドート的に記述します。ここでは詳しくは書きませんが、大変面白い論文なので機会があったら読むことを是非オススメします。

上記の論文では1989年から2000年のデータを利用しています。すると当然のことながら最近のデータを利用すればどうなるか？ということが気になります。このシリーズではそのことを手順を追って見ていきましょう。ただし、あまり隠すのも趣味が悪いので、先に結果だけを提示していきます。コードの詳しい解説は次回以降に回しますが、再現するコードは次のようになります。ここでは、後ほどGoogle Refineでクリーングして手に入れる予定のデータを先取りして利用していて、そのデータはGoogle DocsにCSV形式で置いておきました。

>>>code
df <- read.csv("http://spreadsheets.google.com/pub?key=0AlBuJgqcP5f3dElpb0lWcDRjZldkMzE1LW5aY1VtMHc&hl=en&single=true&gid=0&output=csv", header=FALSE)
names(df) <- c("rikishi", "vs", "win", "year", "month")
nrow(df)
df <- subset(df, df$win!=-100 & df$win!=-1)
library(plyr)
df <- ddply(df, .(rikishi, year, month), summarize, numwin=sum(win))

library(ggplot2)
p2 <- ggplot(df, aes(numwin)) + geom_histogram(aes(y = ..count..))
ggsave(plot = p2, filename = "sumoobservation.png")

dsumo <- function(x, total) total*dbinom(x, size = 15, prob = 1/2)
dft<- data.frame(x = c(0:15), win_theory = dsumo(c(0:15), nrow(df)))
df2 <- as.data.frame(table(df$numwin))
names(df2) <- c("x", "win_observ")
str(df2)
df2 <- transform(df2, x = as.integer(x)-1, win_theory = dft$win_theory)
ggplot(data = melt(df2, id = 'x'), aes(x = x, y = value, colour = variable)) + geom_line()
ggplot(data = df2, aes(x)) + geom_point(aes(y = win_observ, colour = "win_observ")) + geom_line(aes(y = win_observ, colour = "win_observ")) + geom_line(aes(y = win_theory, colour = "win_theory"))
<<<

結果は以下のグラフになります。

>>>img: sumoanorm.png<<<

横軸が勝ち星の数で、縦軸が実際にその勝ち星を稼いだ力士の人数です。赤の線が1999年1月場所から2010年9月場所までの勝敗結果の分布で、青緑の線が勝負がランダムに生じたと仮定したときに描かれる二項分布になります。これをみるとわかるように、本来は500人ほどいてもよい7勝8敗の力士数が、350人程度というようにかなり少なくなっています。また、DugganとLevittの結果よりも、負け星が極端に多い力士と勝ち星が極端に多い力士の人数が多くなっていて、裾が広い分布になっています。このシリーズでは、まずはこの「大相撲のアノーマリー」を自分で確かめることを試みます。

●Google Refine

さて、大相撲の勝敗データさえあれば、それを解析する方法があることは分かっていただけたと思いますが、ここで問題が生じます。大相撲のデータは、世界銀行のRESTインタフェースのような、洗練したデータリポジトリから簡単に入手できるわけではない、ということです。そこで今回は、前述したように、Yahoo! Japanスポーツの大相撲の取り組み結果をスクレイピングして、データを入手することにします。そのために用いるのが、Google Refineです。

http://code.google.com/p/google-refine/>>>link:http://code.google.com/p/google-refine/<<<

>>>img:refine.jpg<<<

Google Refineは汚いデータをクリーニング/クレンジングするツールとしてGoogleのFreebaseチームがリリースしたオープンソース・ソフトウェアです。モットーが"Google Refine, a power tool for working with messy data"とあるように、汚い混乱したデータをキレイにするツールなのです。

Google Refineの特徴を列挙すると以下のようになります。

>>>item
・インタラクティブに結果を見ながら探索的にデータクリーニングができる。
・Google Refineを利用したデータクリーニングの実際は、スプレッドシートにフィルタを書けたり、MS Excelで言うところのピボットテーブルを配置するイメージではあるが、もっと分かりやすくインタラクティブなインタフェースである。
・クリーニングにはスクリプトも利用するが、そのスクリプトにはPython(Jython)、 Closure、あと独自の言語Google Refine Expression Language(GREL)を利用する。
・スクリプトを書けば、リアルタイムにドライランして、その結果を例示してくれる。
・クリーニングの手順はJSONとして記録され、JSONのテキストとして入手できる。それを別のプロジェクトに適用することも可能。
・クリーニングの手順はどのステップにも戻ることができるし、Undo/Redoも自在である。
・スクリプト上にあるオブジェクトはすべてJSON経由で外部のWeb Service APIを利用することができる。例えば住所のカラムがあるときに、それをGoogle Map APIを利用して、セルにある住所から位置情報を取得するということが容易にできる。
・ローカルマシンで動作するwebサーバーであり、クライアントにブラウザを利用する。
・クライアント・サーバー間の通信はWeb技術の標準であるHTTPであり、JSONを利用して構造データを受け渡しする。
・データのクラスタリングアルゴリズムが組み込まれていて、文字列の一致検索だけではなく、類似検索もできる。
・Javaベースである。
・Google Freebaseのスピンアウトプロジェクトであるためか、Freebaseと相性がかなり良く、Freebaseからのインプット・アウトプットが容易である。
<<<

Google Refineはテータクリーニングのために使うために開発されたツールですが、この柔軟で強力なツールをHTMLのwebスクレイピングには使わない手はありません。ここでは、まずはイントロとしてGoogle Refineの手習いをしましょう。

Google Refineのダウンロードは以下のURLから行います。

http://code.google.com/p/google-refine/wiki/Downloads >>>link:http://code.google.com/p/google-refine/wiki/Downloads<<<

ぞれぞれのOSに対応したアーカイブをダウンロードし、アーカイブされたファイルを解凍して適切なフォルダに配置し、Refine.exeをダブルクリック(MacはGoogle Refine.appをダブルクリック、Linuxは"./refine"でコマンドラインから起動すればGoogle Refineのサーバーがローカルマシン上で立ち上がります。後は、http://127.0.0.1:3333/ にアクセスすれば、Google Refineにアクセスできます。

以下では起動できたと仮定して、実際にRefineを使ってYahoo!スポーツの大相撲取り組みデータを「クリーニング」する手順の概略を説明します。といいましても、Refineの操作はほとんどGUIですので、ここで詳細に文章で記述をしてもなかなか掴めないと思います。そこで、今回のRefineを使って探索的にYahoo! Sportsの大相撲取組表をスクレイピングする模様を動画にしましたので、それを御覧ください。

動画：Google Rfefineチュートリアル >>>link:http://www.bakfoo.com/refine.html<<<

動画で使用したHTMLファイルは以下にあります。

https://github.com/yutakashino/atmarkIT/raw/master/sumo/codes/yahooSportsUTF/199901.utf.txt >>>link:https://github.com/yutakashino/atmarkIT/raw/master/sumo/codes/yahooSportsUTF/199901.utf.txt<<<

このファイルをRefineに読み込み、新規プロジェクトとして登録したのちに、以下のJSONスクリプトを「Undo/Redoタブ＞Apply...」ボタンで出現するテキストエリアにコピーして、実行すれば必要なデータを手に入れることができます。

https://github.com/yutakashino/atmarkIT/raw/master/sumo/codes/refineSumoJson.txt >>>link:https://github.com/yutakashino/atmarkIT/raw/master/sumo/codes/refineSumoJson.txt<<<

>>>img:refineApply.jpg<<<

基本的にこのJSONスクリプトがあれば、Yahoo!スポーツの取組表HTMLから必要なデータを抽出できます。しかし、HTMLの数は、1999年1月から2010年9月まで一年あたり7場所ありますから計71個になります。これを手でやるのはさすがに大変なので、これを自動化できないかと考えるのが自然です。

幸いGoogle RefineはHTTPを受け取るWebサーバーですので、HTTPを話すスクリプトでRefineを外から操作することは容易なのです。それをこの後に解説したいのですが、今回は紙幅の関係でここで一旦中断して、HTMLクリーニングの自動化は次回に持ち越しすることにします。

●次回について

次回もこの「大相撲のアノーマリー」をテーマにしたいと思います。次回は、今回行ったGoogle Refineによるクリーニング作業を自動化し、外のスクリプトから操作させ、目的のデータを入手することを中心に行います。それでは、またお会いましょう。